# Detailed difference report: your model (`ENCODER_ROLLING=0`) vs Krischan model (`mode=1`)

## 0) Scope and baseline used for this report

This report compares:

- Your framework temporal path with `ENCODER_ROLLING=0` (binary mode path).
- Krischan model temporal path with runtime `mode=1` (rolling 5-block path).

Code references are from:

- Your code: `hdc_infrastructure/*`, `foot/*`
- Krischan code: `krischans_model/src/*`, `krischans_model/scripts/*`

Important: these are not just two implementations of the same algorithm. They are different pipelines at multiple stages (quantization, temporal composition, training sampling, prototype bundling, and evaluation protocol).

---

## 1) Configuration model and runtime controls

### Your model

- Compile-time macros control behavior (`foot/configFoot.h:4-110`):
  - `VECTOR_DIMENSION`, `NUM_LEVELS`, `N_GRAM_SIZE`, `ENCODER_ROLLING`, `BIPOLAR_MODE`, etc.
- Changing these needs recompilation.

### Krischan model

- `D` and `M` are runtime args (`krischans_model/src/main.c:24-26`).
- Temporal mode is runtime arg (`main.c:26`, `main.c:109-125`, `main.c:157-165`).
- `BLOCK_WINDOW` is fixed to 5 in header (`krischans_model/include/block_accumulator.h:6`).

### Consequence

- Your non-rolling path is compile-time selected and can be made very different from rolling.
- Krischan can switch mode without recompiling, but mode=1 is always a rolling window of 5.

---

## 2) Data loading and preprocessing

### Your model

- Loads train/test from same dataset files (`foot/dataReaderFootEMG.c:14-18`, `:42-52`).
- Optional downsampling (`hdc_infrastructure/preprocessor.c:35-67`) controlled by `DOWNSAMPLE`.
- Optional validation split preserving order (`dataReaderFootEMG.c:76-170`) controlled by `VALIDATION_RATIO`.

### Krischan model

- Loads the same CSV train/test files (`krischans_model/src/main.c:45-52`).
- No downsampling logic.
- No validation split.

### Consequence

- Unless you force `DOWNSAMPLE=1` and `VALIDATION_RATIO=0`, your effective train stream differs from Krischan before encoding starts.

---

## 3) IM/CM generation and memory source

### Your model (native path)

- IM generation in C using `rand()` (`hdc_infrastructure/item_mem.c:77-92`).
- CiM generation as cumulative flips along one permutation with explicit total flip budget (`item_mem.c:158-223`).
- `GA_MAX_FLIPS_CIM` influences max trajectory (`item_mem.c:186`, `:352`, `:455`).

### Krischan model

- IM is generated in Python as random bitstrings (`krischans_model/scripts/randomvector.py`).
- CiM generated by repeatedly flipping `vector_size // 40` random positions per level (`krischans_model/scripts/bitflipvector.py:10`, `:19-21`).
- Flips are with replacement, so repeated toggles can cancel.

### Consequence

- Native IM/CM distributions are structurally different.
- Krischan CiM trajectory is random-walk-like; your CiM is controlled cumulative along a fixed permutation path.

---

## 4) Quantization (feature value -> level)

### Your non-rolling path (`ENCODER_ROLLING=0`)

- Uses min/max normalization and truncation (`hdc_infrastructure/encoder.c:93-101`):
  - clamp by `MIN_LEVEL`, `MAX_LEVEL`
  - `level = (int)(normalized * (NUM_LEVELS - 1))`

### Krischan mode=1

- Uses scaled/offset/ceil quantization (`krischans_model/src/hdc_encode.c:12-20`):
  - `scaled = ceil(x*10000 + 10000)`
  - clamp [0,20000]
  - `level = ((int)scaled * (M - 1) + 10000) / 20000`

### Consequence

- With `ENCODER_ROLLING=0`, your level assignments differ from Krischan unless you explicitly use the Krischan mapping.

---

## 5) Spatial encoding (single timestamp/sample)

### Your model

- For each feature: bind IM(feature) with CiM(level) (`encoder.c:129-137`).
- Bundle across features using majority in `bundle_multi` (`operations.c:150-176`).
- Tie rule at this stage: `>= num_vectors/2` -> tie goes to 1 (`operations.c:175`).

### Krischan model

- Same high-level idea: XOR IM(feature) with CM(level) (`hdc_encode.c:37`).
- Bundle with per-bit majority (`hdc_encode.c:57-67`).
- Tie rule here also goes to 1: `cnt[bit] >= (N/2)` (`hdc_encode.c:66`).

### Consequence

- Spatial bundling tie behavior is aligned.
- Main differences in this stage come from IM/CM source and quantization.

---

## 6) Temporal encoding: core algorithm difference (largest conceptual gap)

### Your `ENCODER_ROLLING=0` temporal encoding

- `encode_timeseries` non-rolling branch (`encoder.c:231-243`):
  - Start with first timestamp HV.
  - Iteratively: `result = bind(permute(result,1), current_timestamp_hv)`.
  - In binary mode, bind is XOR (`operations.c:95`).
- This is recursive n-gram composition, not rolling window accumulation.

### Krischan `mode=1` temporal encoding

- Rolling accumulator (`block_accumulator.c:32-57`):
  - Rotate each current sample HV by `window_pos` (`block_accumulator.c:37`).
  - XOR into rolling accumulator.
  - If window full: XOR out oldest slot, XOR in new slot.
- This is a sliding window XOR over rotated single-sample HVs.

### Consequence

- These are fundamentally different temporal operators.
- Matching IM/CM alone does not make them equivalent.

---

## 7) Permutation/rotation implementation differences

### Your non-rolling path

- Uses index-based circular permutation (`operations.c:198-207`) on unpacked bool vectors.

### Krischan mode=1

- Uses packed-word rotate (`hdc_utils.c:40-50`) on `uint32_t` chunks.
- Loader stores text bits MSB-first within each 32-bit word (`hdc_memory.c:50-54`).

### Additional critical point in Krischan code

- `hv_rotate_right` computes `b << (32 - bit_shift)` even when `bit_shift==0` (`hdc_utils.c:49`), i.e. shift-by-32 on 32-bit type (undefined behavior in C).

### Consequence

- Your non-rolling path does not use this rotate path.
- Krischan mode=1 depends on this behavior every 5th step (`window_pos==0`), which is unsafe and compiler-dependent.

---

## 8) Training sample selection and label assignment

### Your `ENCODER_ROLLING=0` training

- Iterates windows in `train_model_timeseries` non-rolling branch (`trainer.c:179-191`).
- Uses stability check `is_window_stable(labels)` (`trainer.c:181`).
- If unstable, skips ahead by `N_GRAM_SIZE-1` (`trainer.c:189`).
- If stable, uses label at window start (`trainer.c:182`).

### Stability check detail

- `is_window_stable` only compares first and last label (`encoder.c:160-165`), not all labels in window.

### Krischan mode=1 training

- Uses every sample with rolling warm-up (`main.c:103-125`).
- Uses current sample label `y_train[i]` directly (`main.c:105`).
- No stability filter.

### Consequence

- Training data fed to class prototypes differs heavily (which windows exist, stride, labels).
- This alone can change results significantly.

---

## 9) Class prototype construction (AM update)

### Your non-rolling path

- Collect all class n-gram HVs then bundle via `bundle_multi` (`trainer.c:193-200`).
- Class-level tie rule in binary bundle is `>= half` -> tie to 1 (`operations.c:175`).

### Krischan mode=1

- Collects rolling HVs per class then calls `train_class` (`main.c:133-136`).
- Class-level tie rule is strict `> half` -> tie to 0 (`hdc_train.c:13-16`).

### Consequence

- Even with same candidate vectors, ties at class-prototype level resolve differently.

---

## 10) Classification metric and tie behavior

### Your model

- Similarity function in binary is normalized Hamming similarity (`operations.c:263-273`, `:286-298`).
- Classifier selects max similarity (`assoc_mem.c:126-136`).

### Krischan model

- Uses raw Hamming distance and picks minimum (`hdc_classify.c:10-15`).

### Consequence

- Mathematically equivalent ranking (monotonic transform), tie behavior effectively same (first class wins).
- This stage is usually not a major source of mismatch.

---

## 11) Evaluation protocol mismatch

### Your `ENCODER_ROLLING=0` evaluation

- Evaluates non-overlapping windows: `j += N_GRAM_SIZE` (`evaluator.c:319`).
- Ground truth label per window is mode over labels (`evaluator.c:320`).
- Tracks transition errors (`evaluator.c:342-346`) and includes them in total (`evaluator.c:349`).
- Reports transition-excluded accuracy as additional metric (`evaluator.c:356-363`).

### Krischan mode=1 evaluation

- Evaluates every sample after warm-up (`main.c:153-165`).
- Ground truth is per-sample label `y_test[i]` (`main.c:167`).
- Accuracy denominator is full `test_count` even though first 4 samples are skipped (`main.c:170`).

### Consequence

- Different prediction frequency, different label semantics, different denominator.
- Reported accuracies are not directly comparable as-is.

---

## 12) Numeric representation and performance model

### Your model

- Vector type is `bool` per dimension in binary mode (`hdc_infrastructure/vector.h`).
- Most operations are scalar per-bit loops over `VECTOR_DIMENSION`.

### Krischan model

- Vector type is packed `uint32_t*` (`krischans_model/include/hdc_types.h`).
- Operations exploit word-level XOR and popcount (`hdc_utils.c:22-33`).

### Consequence

- Krischan implementation is much faster per operation.
- Different low-level representations also interact with rotation semantics and bit ordering.

---

## 13) Determinism and reproducibility differences

### Your model

- Some item memory generators still use C `rand()` (`item_mem.c:87-90`, `:113-117`, `:142`, `:179`).
- Without `srand`, C runtime defaults are typically deterministic across runs on same platform.
- Other newer paths use local xorshift RNG.

### Krischan model

- IM/CM scripts use Python `random` without fixed seed (`randomvector.py`, `bitflipvector.py`).
- Regeneration usually changes memories each run unless seed is fixed externally.

### Consequence

- Re-run variance is typically higher in Krischan memory generation workflow unless explicitly seeded.

---

## 14) "Wrong" vs "different" summary (for discussion with Krischan)

### Objectively problematic (can be called wrong)

1. Undefined behavior in rotate for `bit_shift==0` (`krischans_model/src/hdc_utils.c:49`).
2. Accuracy denominator in mode=1 includes warm-up skipped samples (`main.c:162-163`, `:170`).

### Major design differences (not automatically wrong, but not equivalent to your non-rolling model)

1. Different temporal operator (recursive n-gram bind/permute vs rolling XOR window).
2. Different training sampling and label policy (stability-filtered n-grams vs all samples post warm-up).
3. Different class-prototype tie rule (`>=` vs `>` at class aggregation).
4. Different quantization formula when your `ENCODER_ROLLING=0` is active.
5. Different evaluation protocol (window-mode labels vs per-sample labels).

### Potentially problematic on your side (for fairness)

1. `is_window_stable` checks only first and last labels, not full window (`encoder.c:160-165`).
2. Train/eval protocols are internally inconsistent in non-rolling path (overlap in training, stride-by-N in eval).

---

## 15) Net conclusion

Comparing your `ENCODER_ROLLING=0` results directly to Krischan `mode=1` results is not an apples-to-apples comparison.
They differ in:

- memory generation process,
- quantization,
- temporal encoding algorithm,
- training sample/label policy,
- class bundling tie rule,
- and evaluation definition.

So performance differences are expected and cannot be attributed to one single implementation detail.
